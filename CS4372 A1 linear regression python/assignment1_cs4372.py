# -*- coding: utf-8 -*-
"""Assignment1_CS4372.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FcyO4ySm0xq43F5dGssblY8CzCRVgufJ

John Kenney- jfk150030, Matt Brown - meb180001

Project Selection: California Housing Prices
"""

#importing the libraries
import numpy as np  
import pandas as pd 


# visualize dataset
import seaborn as sns 
import matplotlib.pyplot as plt

# used to standardize/normalize attributes
from sklearn.preprocessing import StandardScaler

# linear model w/ metrics
from sklearn.linear_model import SGDRegressor
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score


# used to split training and test data
from sklearn.model_selection import train_test_split

"""Set Parameters here for model"""

#loss = The loss function to be used. The possible values are ‘squared_loss’, ‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’
#penalty = ‘l2’, ‘l1’, ‘elasticnet’
#alpha = Constant that multiplies the regularization term
#learning rate = 'constant’, ‘optimal’, ‘invscaling’, ‘adaptive’
#eta0 = The initial learning rate for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules. The default value is 0.01.
loss_func = 'squared_loss'
pen_func = 'l1'
pen_alph = 0.0001
M_iter = 150
lr = 'adaptive'
ini_lr = 0.1

"""Importing and preprocessing the data

"""

# To download file from GitHub public url
raw_url = "https://raw.githubusercontent.com/mattebrown95/CS_4372/main/housing_Assignment1.csv"

# import the dataset into a variable
housing_dataset = pd.read_csv(raw_url)
# verify data read into pandas df
# print(type(housing_dataset))

# look at the dataset
#housing_dataset.head(10)

# understand the dataset
# housing_dataset.describe()
# returns 20640 rows * 10 columns
# print(housing_dataset.shape)

# look at column names
# print(housing_dataset.columns)

"""Checking for missing data values. Examine data for consistency."""

# Drop rows that have one or more null values
housing_dataset = housing_dataset.dropna()
# verify ZERO nulls present
print(housing_dataset.isnull().values.sum())

max(housing_dataset['median_house_value'])

"""We see that 500001 are outliers may have capped median at this number for some reason"""

sns.displot(housing_dataset, x="median_house_value", kde=True)



housing_dataset = housing_dataset[housing_dataset['median_house_value'] < max(housing_dataset['median_house_value'])]

sns.displot(housing_dataset, x="median_house_value", kde=True)



X = housing_dataset
cols_to_drop = ['longitude', 'latitude']
X = X.drop(labels=cols_to_drop, axis=1)

#cols_to_drop = [ 'total_bedrooms', 'population', 'median_house_value', 'ocean_proximity']
#temp = X.drop(labels=cols_to_drop,axis = 1)
sns.displot(housing_dataset, x="housing_median_age", kde=True)
#ax = sns.displot(data=temp)

sns.boxplot(x=housing_dataset["housing_median_age"])

sns.displot(housing_dataset, x="total_rooms", kde=True)

sns.boxplot(x=housing_dataset["total_rooms"])



sns.displot(housing_dataset, x="households", kde=True)

sns.boxplot(x=housing_dataset["households"])

sns.displot(housing_dataset, x="median_income", kde=True)

sns.boxplot(x=housing_dataset["median_income"])

"""Assign predictors and response variables

Examine attributes and target variables. Deal with categorical variables (ocean proximity).
"""

# determine # of distinct values for ocean_proximity
print(X['ocean_proximity'].value_counts().count())

# determine values of each variable for ocean_proximity
print(X['ocean_proximity'].value_counts())

# visualize the data values for ocean_proximity
ocean_proximity_count = X['ocean_proximity'].value_counts()
sns.set(style="darkgrid")
sns.barplot(ocean_proximity_count.index, ocean_proximity_count.values, alpha=0.9)
plt.title('Distribution of Ocean Proximity for California Homes')
plt.xlabel('Ocean Proximity', fontsize=10)
plt.ylabel('Number of Homes', fontsize=10)
plt.show()

#print(X.dtypes)

X = pd.get_dummies(X, dtype=float)
# These will not be helpful in predicting the value of a home for our assignment
# there are only 5 houses out of 20000 in this category

X

# understand the relationship b/t the variables
correlation_matrix = X.corr().round(2)
sns.set(rc={'figure.figsize':(11.7,8.27)})
sns.heatmap(data=correlation_matrix, annot=True)

#print(X.keys())

cols_to_drop = [ 'total_bedrooms', 'population', 'median_house_value', 'ocean_proximity_<1H OCEAN', 'ocean_proximity_ISLAND', 'ocean_proximity_NEAR BAY', 'ocean_proximity_NEAR OCEAN']
X = X.drop(labels=cols_to_drop, axis=1)
column_names = X.keys().tolist()
#type(column_names)
#print(X.keys())

# drop response for X
#X = X.drop(labels=['median_house_value'], axis=1)
#print(X)
#print(X.shape)
#X = np.array(X)
#print(X)
#print(X.shape)
# response is home value
Y = housing_dataset['median_house_value']
#print(Y)
#print(Y.shape)
#Y = np.array(Y).reshape(len(Y))
#print(Y)
#print(Y.shape)

"""Standardize the variables, change column names to represent original predictors"""

ss = StandardScaler()
X = pd.DataFrame(ss.fit_transform(X.values))
X.columns = column_names
# print(X.columns)

"""Split data into training and test parts. Training: 80%, Test: 20%"""

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=5)
#Y_train = np.array(Y_train).reshape((len(Y_train)))
#Y_test = np.array(Y_test).reshape((len(Y_test)))
print(X_train.shape)
print(X_test.shape)
print(Y_train.shape)
print(Y_test.shape)

"""Construct model and tune parameters."""

reg = SGDRegressor(loss = loss_func,penalty= pen_func,alpha =pen_alph, max_iter = M_iter,random_state=1,learning_rate = lr,eta0=ini_lr)
reg.fit(X_train, Y_train)

reg.coef_

"""Determine best way to interpret this value."""

reg.intercept_

"""Pair the column names with their corresponding weights, see which variables have the most power in predicting the median home value."""

labeled_weights = dict(zip(X.columns,reg.coef_))
print(labeled_weights)

import operator

max_key = max(labeled_weights.items(), key=operator.itemgetter(1))[0]
max_value = max(labeled_weights.items(), key=operator.itemgetter(1))[1]
min_key = min(labeled_weights.items(), key=operator.itemgetter(1))[0]
min_value = min(labeled_weights.items(), key=operator.itemgetter(1))[1]
print("\nMost correlated:", max_key, max_value)
print("\nLeast correlated:", min_key, min_value)

"""Trying the statsmodels API for ordinary least squares.
Look at the relationship between income and home value.
"""

y_train_predict = reg.predict(X_train)
rmse_train = (np.sqrt(mean_squared_error(Y_train, y_train_predict)))
r2_train = r2_score(Y_train, y_train_predict)

print("The model performance for training set")
print("--------------------------------------")
print('RMSE is {}'.format(rmse_train))
print('R2 score is {}'.format(r2_train))





error_train = Y_train - y_train_predict
s = sns.relplot(x=y_train_predict, y=error_train,size=np.abs(error_train),hue=np.abs(error_train),legend=True);
s._legend.set_title('abs(error_train)')
s.set_axis_labels("Predicted median house values of train set","Residuals of train set")
s.fig.suptitle('Residuals vs predicted Median house values of train set', fontsize=14)
s.fig.subplots_adjust(top=.9);

s = sns.relplot(x=X_train['housing_median_age'], y=error_train,size=np.abs(error_train),hue=np.abs(error_train),legend=True);
s._legend.set_title('abs(error_train)')
s.set_axis_labels("housing_median_age of train set","Residuals of train set")
s.fig.suptitle('Residuals vs housing_median_age of train set', fontsize=14)
s.fig.subplots_adjust(top=.9);

s = sns.relplot(x=X_train['total_rooms'], y=error_train,size=np.abs(error_train),hue=np.abs(error_train),legend=True);
s._legend.set_title('abs(error_train)')
s.set_axis_labels("total_rooms of train set","Residuals of train set")
s.fig.suptitle('Residuals vs total_rooms of train set', fontsize=14)
s.fig.subplots_adjust(top=.9);

s = sns.relplot(x=X_train['households'], y=error_train,size=np.abs(error_train),hue=np.abs(error_train),legend=True);
s._legend.set_title('abs(error_train)')
s.set_axis_labels("households of train set","Residuals of train set")
s.fig.suptitle('Residuals vs households of train set', fontsize=14)
s.fig.subplots_adjust(top=.9);

s = sns.relplot(x=X_train['median_income'], y=error_train,size=np.abs(error_train),hue=np.abs(error_train),legend=True);
s._legend.set_title('abs(error_train)')
s.set_axis_labels("median_income of train set","Residuals of train set")
s.fig.suptitle('Residuals vs median_income of train set', fontsize=14)
s.fig.subplots_adjust(top=.9);

s = sns.relplot(x=X_train['ocean_proximity_INLAND'], y=error_train,size=np.abs(error_train),hue=np.abs(error_train),legend=True);
s._legend.set_title('abs(error_train)')
s.set_axis_labels("ocean_proximity_INLAND of train set","Residuals of train set")
s.fig.suptitle('Residuals vs ocean_proximity_INLAND of train set', fontsize=14)
s.fig.subplots_adjust(top=.9);

"""Apply the model on training and test datasets, report the diagnostic parameters."""

y_test_predict = reg.predict(X_test)
rmse_test = (np.sqrt(mean_squared_error(Y_test, y_test_predict)))
r2_test = r2_score(Y_test, y_test_predict)

print("The model performance for testing set")
print("--------------------------------------")
print('RMSE is {}'.format(rmse_test))
print('R2 score is {}'.format(r2_test))



error_test = Y_test - y_test_predict
s = sns.relplot(x=y_test_predict, y=error_test,size=np.abs(error_test),hue=np.abs(error_test),legend=True);
s._legend.set_title('abs(error_test)')
s.set_axis_labels("Predicted Median house values of test set","Residuals of test set")
s.fig.suptitle('Residuals vs predicted Median house values of test set', fontsize=14)
s.fig.subplots_adjust(top=.9);





s = sns.relplot(x=X_test['housing_median_age'], y=error_test,size=np.abs(error_test),hue=np.abs(error_test),legend=True);
s._legend.set_title('abs(error_test)')
s.set_axis_labels("housing_median_age of test set","Residuals of test set")
s.fig.suptitle('Residuals vs housing_median_age of test set', fontsize=14)
s.fig.subplots_adjust(top=.9);

s = sns.relplot(x=X_test['total_rooms'], y=error_test,size=np.abs(error_test),hue=np.abs(error_test),legend=True);
s._legend.set_title('abs(error_test)')
s.set_axis_labels("total_rooms of test set","Residuals of test set")
s.fig.suptitle('Residuals vs total_rooms of test set', fontsize=14)
s.fig.subplots_adjust(top=.9);

s = sns.relplot(x=X_test['households'], y=error_test,size=np.abs(error_test),hue=np.abs(error_test),legend=True);
s._legend.set_title('abs(error_test)')
s.set_axis_labels("households of test set","Residuals of test set")
s.fig.suptitle('Residuals vs households of test set', fontsize=14)
s.fig.subplots_adjust(top=.9);

s = sns.relplot(x=X_test['median_income'], y=error_test,size=np.abs(error_test),hue=np.abs(error_test),legend=True);
s._legend.set_title('abs(error_test)')
s.set_axis_labels("median_income of test set","Residuals of test set")
s.fig.suptitle('Residuals vs median_income of test set', fontsize=14)
s.fig.subplots_adjust(top=.9);

s = sns.relplot(x=X_test['ocean_proximity_INLAND'], y=error_test,size=np.abs(error_test),hue=np.abs(error_test),legend=True);
s._legend.set_title('abs(error_test)')
s.set_axis_labels("ocean_proximity_INLAND of test set","Residuals of test set")
s.fig.suptitle('Residuals vs ocean_proximity_INLAND of test set', fontsize=14)
s.fig.subplots_adjust(top=.9);

#print(labeled_weights)

#reg.coef_[0]

print("--------------------------------------")
print("--------------------------------------")
print(f'New Run with New parameters:')
print(f'Parameters used on the model: Loss Function "{loss_func}", Penalty function: "{pen_func}", alpha: "{pen_alph}", Max Iterations: "{M_iter}", Learning rate: "{lr}", Initial learning rate: "{ini_lr}"')
print("--------------------------------------")
print(f'Model equation: y_hat = ({reg.intercept_[0]})*intercept + ({reg.coef_[0]})*housing_median_age + ({reg.coef_[1]})*total_rooms + ({reg.coef_[2]})*households + ({reg.coef_[3]})*median_income + ({reg.coef_[4]})*ocean_proximity_INLAND' )
print("--------------------------------------")
print("The model performance for training set")
print('RMSE is {}'.format(rmse_train))
print('R2 score is {}'.format(r2_train))
print("--------------------------------------")
print("The model performance for testing set")
print('RMSE is {}'.format(rmse_test))
print('R2 score is {}'.format(r2_test))

