# -*- coding: utf-8 -*-
"""hw2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pIsDj1yWuVT0r0yrdLN7KAgqeCoJpvDF
"""

#####################################################################################################################
#   Assignment 2: Neural Network Programming
#   This is a starter code in Python 3.6 for a 1-hidden-layer neural network.
#   You need to have numpy and pandas installed before running this code.
#   Below are the meaning of symbols:
#   NeuralNet class init method takes file path as parameter and splits it into train and test part
#         - it assumes that the last column will the label (output) column
#   h - number of neurons in the hidden layer
#   X - vector of features for each instance
#   y - output for each instance
#   W_hidden - weight matrix connecting input to hidden layer
#   Wb_hidden - bias matrix for the hidden layer
#   W_output - weight matrix connecting hidden layer to output layer
#   Wb_output - bias matrix connecting hidden layer to output layer
#   deltaOut - delta for output unit (see slides for definition)
#   deltaHidden - delta for hidden unit (see slides for definition)
#   other symbols have self-explanatory meaning
#   You need to complete all TODO marked sections
#   You are free to modify this code in any way you want, but need to mention it in the README file.
#
#####################################################################################################################


import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plot
import plotly.graph_objects as go

class NeuralNet:
    def __init__(self, dataFile,m_iter,funct,lr,h,tr, header=True):
        #np.random.seed(1)
        # train refers to the training dataset
        # test refers to the testing dataset
        # h represents the number of neurons in the hidden layer

        #### intialize parameters
        self.max_iter = m_iter
        self.function = funct
        self.learning__rate = lr
        self.training_size = tr

        self.MSE_training = 0
        self.acc_training = 0
        self.MSE_test = 0
        self.acc_test = 0
        self.mse_log = np.zeros((self.max_iter,1))
        self.acc_log = np.zeros((self.max_iter,1))
        # TODO: Remember to implement the preprocess method
        self.train_dataset, self.test_dataset = train_test_split(dataFile,train_size = self.training_size)

        ncols = len(self.train_dataset.columns)
        nrows = len(self.train_dataset.index)
        self.X = self.train_dataset.iloc[:, 0:(ncols -1)].values.reshape(nrows, ncols-1)
        self.y = self.train_dataset.iloc[:, (ncols-1)].values.reshape(nrows, 1)

        ncols_test = len(self.test_dataset.columns)
        nrows_test = len(self.test_dataset.index)
        self.X_test = self.test_dataset.iloc[:, 0:(ncols_test -1)].values.reshape(nrows_test, ncols_test-1)
        self.y_test = self.test_dataset.iloc[:, (ncols_test-1)].values.reshape(nrows_test, 1)
        #
        # Find number of input and output layers from the dataset
        #
        input_layer_size = len(self.X[1])
        if not isinstance(self.y[0], np.ndarray):
            self.output_layer_size = 1
        else:
            self.output_layer_size = len(self.y[0])
        
        # assign random weights to matrices in network
        # number of weights connecting layers = (no. of nodes in previous layer) x (no. of nodes in following layer)
        self.W_hidden = 2 * np.random.random((input_layer_size, h)) - 1
        self.Wb_hidden = 2 * np.random.random((1, h)) - 1

        self.W_output = 2 * np.random.random((h, self.output_layer_size)) - 1
        self.Wb_output = np.ones((1, self.output_layer_size))

        self.deltaOut = np.zeros((self.output_layer_size, 1))
        self.deltaHidden = np.zeros((h, 1))
        self.h = h

    #
    # TODO: I have coded the sigmoid activation function, you need to do the same for tanh and ReLu
    #

    def __activation(self, x):
        if self.function == "sigmoid":
            self.__sigmoid(self, x)
        if self.function == "tanh":
            self.__tanh(self, x)
        if self.function == "ReLu":
            self.__ReLu(self, x)

    #
    # TODO: Define the derivative function for tanh, ReLu and their derivatives
    #

    def __activation_derivative(self, x):
        if self.function == "sigmoid":
            self.__sigmoid_derivative(self, x)
        if self.function == "tanh":
            self.__tanh_derivative(self, x)
        if self.function == "ReLu":
            self.__ReLu_derivative(self, x)

    def __sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def __tanh(self, x):
        return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))
    
    def __ReLu(self,x):
        x2 = np.where(x > 0, x, 0.0)
        return x2
    # derivative of sigmoid function, indicates confidence about existing weight

    def __sigmoid_derivative(self, x):
        return x * (1 - x)

    def __tanh_derivative(self, x):
        return (1 - np.power(x,2))

    def __ReLu_derivative(self, x):
        return np.where(x > 0, 1.0, 0.0)

    # Below is the training function

    def train(self):
        for iteration in range(self.max_iter):
            out = self.forward_pass(self.X)
            error = 0.5 * np.power((out - self.y), 2)
            self.mse_log[iteration] = mean_squared_error(self.y, self.evaluate(out))
            self.acc_log[iteration] = self.accuracy(self.evaluate(out), self.y)
            # TODO: I have coded the sigmoid activation, you have to do the rest
            self.backward_pass(out)
            update_weight_output = self.learning__rate * np.dot(self.X_hidden.T, self.deltaOut)
            update_weight_output_b = self.learning__rate * np.dot(np.ones((np.size(self.X, 0), 1)).T, self.deltaOut)

            update_weight_hidden = self.learning__rate * np.dot(self.X.T, self.deltaHidden)
            update_weight_hidden_b = self.learning__rate * np.dot(np.ones((np.size(self.X, 0), 1)).T, self.deltaHidden)

            self.W_output += update_weight_output
            self.Wb_output += update_weight_output_b
            self.W_hidden += update_weight_hidden
            self.Wb_hidden += update_weight_hidden_b
            
        out = self.forward_pass(self.X)
        self.MSE_training = mean_squared_error(self.y, self.evaluate(out))
        self.acc_training = self.accuracy(self.evaluate(out), self.y)
        print("After " + str(self.max_iter) + " iterations, the total error is " + str(np.sum(error)))
        print("The final weight vectors are (starting from input to output layers) \n" + str(self.W_hidden))
        print("The final weight vectors are (starting from input to output layers) \n" + str(self.W_output))

        print("The final bias vectors are (starting from input to output layers) \n" + str(self.Wb_hidden))
        print("The final bias vectors are (starting from input to output layers) \n" + str(self.Wb_output))

    def forward_pass(self, temp):
        # pass our inputs through our neural network
        in_hidden = np.dot(temp, self.W_hidden) + self.Wb_hidden
        # TODO: I have coded the sigmoid activation, you have to do the rest
        if self.function == "sigmoid":
            self.X_hidden = self.__sigmoid(in_hidden)
        if self.function == "tanh":
            self.X_hidden = self.__tanh(in_hidden)
        if self.function == "ReLu":
            self.X_hidden = self.__ReLu(in_hidden)
        in_output = np.dot(self.X_hidden, self.W_output) + self.Wb_output
        if self.function == "sigmoid":
            out = self.__sigmoid(in_output)
        if self.function == "tanh":
            out = self.__tanh(in_output)
        if self.function == "ReLu":
            out = self.__ReLu(in_output)
        return out

    def backward_pass(self, out):
        # pass our inputs through our neural network
        self.compute_output_delta(out)
        self.compute_hidden_delta()

    
    def plot_MSE_V_iterations(self,header = True):
      a = np.array(np.arange(self.max_iter).reshape(self.max_iter,1))
      plot.scatter(a,self.mse_log)
      plot.grid()
      plot.xlabel('iterations')
      plot.ylabel('MSE')
      plot.title('iterations vs MSE during training')
      plot.show()

    def plot_accuracy_V_iterations(self,header = True):
      a = np.array(np.arange(self.max_iter).reshape(self.max_iter,1))
      plot.scatter(a,self.acc_log)
      plot.grid()
      plot.xlabel('iterations')
      plot.ylabel('Accuracy')
      plot.title('iterations vs Accuracy during training')
      plot.show()

    def table(self):
      fig = go.Figure(data=[go.Table(header=dict(values=['function','max iterations', 'training_size','learning_rate','neurons_hidden','MSE of training data','MSE of test data','acc of training data','acc of test data']),
                 cells=dict(values=[[self.function], [self.max_iter],[self.training_size],[self.learning__rate],[self.h],[round(self.MSE_training,5)],[round(self.MSE_test,5)],[round(self.acc_training,5)],[round(self.acc_test,5)]]))
                     ])
      fig.show()
        
    def variables(self):
      print('{:7s} {:5d}   {:03.2f}   {:3.0e}  {:3d}    {:06.5f}   {:06.5f}   {:06.5f}   {:06.5f}'.format(self.function,self.max_iter,self.training_size,self.learning__rate,self.h,round(self.MSE_training,5),round(self.MSE_test,5),round(self.acc_training,5),round(self.acc_test,5)))

    def compute_output_delta(self, out):
        if self.function == "sigmoid":
            delta_output = (self.y - out) * (self.__sigmoid_derivative(out))
        if self.function == "tanh":
            delta_output = (self.y - out) * (self.__tanh_derivative(out))
        if self.function == "ReLu":
            delta_output = (self.y - out) * (self.__ReLu_derivative(out))
        self.deltaOut = delta_output

    def compute_hidden_delta(self):
        if self.function == "sigmoid":
            delta_hidden_layer = (self.deltaOut.dot(self.W_output.T)) * (self.__sigmoid_derivative(self.X_hidden))
        if self.function == "tanh":
            delta_hidden_layer = (self.deltaOut.dot(self.W_output.T)) * (self.__tanh_derivative(self.X_hidden))
        if self.function == "ReLu":
            delta_hidden_layer = (self.deltaOut.dot(self.W_output.T)) * (self.__ReLu_derivative(self.X_hidden))
        self.deltaHidden = delta_hidden_layer

    def evaluate(self,x):
      return np.where(x >= .5, 1.0, 0.0)
    
    def accuracy(self,x,y):
      return (x == y).mean()
    # TODO: Implement the predict function for applying the trained model on the  test dataset.
    # You can assume that the test dataset has the same format as the training dataset
    # You have to output the test error from this function

    def predict(self, header = True):
        # TODO: obtain prediction on self.test_dataset
        out = self.forward_pass(self.X_test)
        self.acc_test = self.accuracy(self.evaluate(out),self.y_test)
        self.MSE_test = mean_squared_error(self.y_test,self.evaluate(out))
        return self.MSE_test

def preprocessData(data):
  datafile = pd.read_csv(data, sep = "	", header=None, names=["x1", "x2", "x3","y"])
  datafile.dropna()
  datafile.iloc[:,[3]] = datafile.iloc[:,[3]] - 1
  return datafile

# perform pre-processing of both training and test part of the test_dataset
# split into train and test parts if needed
#####################################################
###################################################
###################################################
_function = "sigmoid"
#_function = "tanh"
#_function = "ReLu"
_max__iterations = 200
_training_size = .8
__learning_rate = .0000001
neurons_hidden = 12
df = preprocessData("https://archive.ics.uci.edu/ml/machine-learning-databases/00229/Skin_NonSkin.txt")
neural_network = NeuralNet(df,_max__iterations,_function,__learning_rate,neurons_hidden,_training_size)
neural_network.train()
neural_network.plot_MSE_V_iterations()
neural_network.plot_accuracy_V_iterations()
testError = neural_network.predict()
#neural_network.table()

print("Test MSE = " + str(testError))
neural_network.variables()

